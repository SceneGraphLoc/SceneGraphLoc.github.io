<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="3D Semantic Instance Segmentation with Open-Vocabulary queries">
  <meta name="keywords" content="OpenMask3D, Instance Segmentation, 3D Scene Understanding">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SceneGraphLoc</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">SceneGraphLoc <br />
            <p class="title is-3 publication-title">Cross-Modal Coarse Visual Localization on 3D Scene Graphs </p>
          </h1>          

          <!-- <h1 class="title is-4" style="color: #5c5c5c;">NeurIPS 2023</h1> -->

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://y9miao.github.io/">Yang Miao</a><sup>1</sup>, 
            </span>
            <span class="author-block">
              <a href="https://cvg.ethz.ch/team/Dr-Francis-Engelmann">Francis Engelmann</a><sup>1, 2</sup>, 
            </span>
            <span class="author-block">
              <a href="https://cvg.ethz.ch/team/Dr-Olga-Vysotska">Olga Vysotska</a><sup>1, 2</sup>, 
            </span>
            <span class="author-block">
              <a href="https://federicotombari.github.io/"> Federico Tombari</a><sup>2, 3</sup>, 
            </span>
            <span class="author-block">
              <a href="https://cvg.ethz.ch/team/Prof-Dr-Marc-Pollefeys"> Marc Pollefeys</a><sup>1, 4</sup>, 
            </span>
            <span class="author-block">
              <a href="https://cvg.ethz.ch/team/Dr-Daniel-Bela-Barath"> Dániel Béla Baráth</a> <sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block" style="margin-right: 1em;"><sup>1</sup>ETH Zürich</span>
            <span class="author-block" style="margin-right: 1em;"><sup>2</sup>Google</span>
            <span class="author-block" style="margin-right: 1em;"><sup>3</sup>TU Munich</span>
            <span class="author-block" style="margin-right: 1em;"><sup>3</sup>Microsoft</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="http://arxiv.org/abs/2404.00469"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Preprint</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=_7YPGsMrVcQ"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/y9miao/VLSG"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (release upon acceptance)</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
      <strong>SceneGraphLoc</strong> solves the novel problem of cross-modal localization of a query image within 3D scene graphs incorporating a mixture of modalities.
    </h2>
    </div>
  </div>
</section>

    <!--/ Abstract. -->
    <style>
      .video-container {
        max-width: 768px !important; /* Maximum video width */
        margin: auto !important; /* Center the video */
      }
      
      .video-container video {
        width: 100% !important; /* Make the video fill its container */
        height: auto !important; /* Scale the height according to the width to maintain aspect ratio */
      }
      </style>
    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Explanatory Video</h2>
        <div class="video-container">
          <video controls poster="static/images/VideoPoster.jpg">
            <source src="static/videos/SceneGraphLoc.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>
      </div>
    </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We introduce a novel problem, i.e., the localization of an input image within a multi-modal reference map represented by a database of 3D scene graphs. 
            These graphs comprise multiple modalities, including object-level point clouds, images, attributes, and relationships between objects, offering a lightweight and efficient alternative to conventional methods that rely on extensive image databases. 
            Given the available modalities, the proposed method SceneGraphLoc learns a fixed-sized embedding for each node (i.e., representing an object instance) in the scene graph, enabling effective matching with the objects visible in the input query image. 
            This strategy significantly outperforms other cross-modal methods, even without incorporating images into the map embeddings. 
            When images are leveraged, SceneGraphLoc achieves performance close to that of state-of-the-art techniques depending on large image databases, while requiring three orders-of-magnitude less storage and operating orders-of-magnitude faster. 
            The code will be made public.        
          </p>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Framework</h2>
        <div class="content has-text-justified">
          <img src="static/images/FrameWork.png" class="interpolation-image">
          <p>
            The training phase is represented by orange arrows, while blue arrows denote the inference phase. 
            During training, a query image and its associated 3D scene graph form a positive sample within a contrastive learning framework, where
              negative samples are generated by associating scene graphs of different scenes with the same query image. 
            The objective is to learn the embeddings of both the graph and the image so that embeddings of the positive pair are drawn closer, 
              whereas those of the negative pair are pushed apart. 
            In the inference phase, the task involves assigning the correct scene graph to a given query image from a selection of multiple graphs,
              achieved by optimizing the cosine similarity between their embeddings.     
          </p>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Multi-Modal Scene Graph</h2>
        <div class="content has-text-justified">
          <img src="static/images/SceneGraphDefinition.png" class="interpolation-image">
          <p>
            Our method considers the scene graph containing a set of objects as nodes, and their relationships as edges,
            with modalities including 3D point cloud, 2D images crops and text descriptions of objects, and relationships between them.
          </p>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Multi-Modal Embedding</h2>
        <div class="content has-text-justified">
          <img src="static/images/MultiModalEmb.png" class="interpolation-image">
          <p>
            For scene graph embedding, we leverage the rich information of multiple modalities of the 3D scene graph,
            and encode each modality separately and then fuse them together to get the final unified embedding for each object node.
          </p>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Qualitative results</h2>
        <div class="content has-text-justified">
          <img src="static/images/QualitativeResults.png" class="interpolation-image">
          <p>
            By leveraging multi-modal information of the 3D scene graph, 
            we associate patch-object matching between the query image and the objects within the scene graph 
              and calculate image-scene similarity score for scene retrieval.
          </p>
        </div>
      </div>
    </div>



  <h2 class="title is-3">BibTeX</h2>
  <pre><code>
    @misc{miao2024scenegraphloc,
      title={SceneGraphLoc: Cross-Modal Coarse Visual Localization on 3D Scene Graphs}, 
      author={Yang Miao and Francis Engelmann and Olga Vysotska and Federico Tombari and Marc Pollefeys and Dániel Béla Baráth},
      year={2024},
      eprint={2404.00469},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
    }
  </code></pre>

</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
              href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            It borrows the source code of <a href="https://github.com/nerfies/nerfies.github.io">this website</a>.
            We would like to thank Utkarsh Sinha and Keunhong Park.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>


